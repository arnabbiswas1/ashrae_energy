{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATED_DATA_DIR = '/home/jupyter/kaggle/energy/data/read_only_feather/v1'\n",
    "\n",
    "def read_data(data_dir):\n",
    "    print('Reading Data...')\n",
    "    train_df = feather.read_dataframe(f'{data_dir}/train_merged.feather')\n",
    "    test_df = feather.read_dataframe(f'{data_dir}/test_merged.feather')\n",
    "    print(f'Shape of train_df : {train_df.shape}')\n",
    "    print(f'Shape of test_df : {test_df.shape}')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def create_date_features(df, feature_name):\n",
    "    '''\n",
    "    Create new features related to dates\n",
    "    \n",
    "    df : The complete dataframe\n",
    "    feature_name : Name of the feature of date type which needs to be decomposed.\n",
    "    '''\n",
    "    df.loc[:, 'year'] = df.loc[:, feature_name].dt.year.astype('uint32')\n",
    "    df.loc[:, 'month'] = df.loc[:, feature_name].dt.month.astype('uint32')\n",
    "    df.loc[:, 'quarter'] = df.loc[:, feature_name].dt.quarter.astype('uint32')\n",
    "    df.loc[:, 'weekofyear'] = df.loc[:, feature_name].dt.weekofyear.astype('uint32')\n",
    "    \n",
    "    df.loc[:, 'day'] = df.loc[:, feature_name].dt.day.astype('uint32')\n",
    "    df.loc[:, 'dayofweek'] = df.loc[:, feature_name].dt.dayofweek.astype('uint32')\n",
    "    df.loc[:, 'dayofyear'] = df.loc[:, feature_name].dt.dayofyear.astype('uint32')\n",
    "    df.loc[:, 'is_month_start'] = df.loc[:, feature_name].dt.is_month_start\n",
    "    df.loc[:, 'is_month_end'] = df.loc[:, feature_name].dt.is_month_end\n",
    "    df.loc[:, 'is_quarter_start']= df.loc[:, feature_name].dt.is_quarter_start\n",
    "    df.loc[:, 'is_quarter_end'] = df.loc[:, feature_name].dt.is_quarter_end\n",
    "    df.loc[:, 'is_year_start'] = df.loc[:, feature_name].dt.is_year_start\n",
    "    df.loc[:, 'is_year_end'] = df.loc[:, feature_name].dt.is_year_end\n",
    "    \n",
    "    df.loc[:, 'hour'] = df.loc[:, feature_name].dt.hour.astype('uint32')\n",
    "    df.loc[:, 'minute'] = df.loc[:, feature_name].dt.minute.astype('uint32')\n",
    "    df.loc[:, 'second'] = df.loc[:, feature_name].dt.second.astype('uint32')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/py37/lib/python3.7/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_df : (20216100, 16)\n",
      "Shape of test_df : (41697600, 16)\n",
      "Shape of train : (20216100, 16)\n",
      "Shape of train : (41697600, 16)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = read_data(CREATED_DATA_DIR)\n",
    "\n",
    "train_ordered_column_names = ['site_id', 'building_id', 'timestamp', 'meter',\n",
    "       'primary_use', 'square_feet', 'year_built', 'floor_count',\n",
    "       'air_temperature', 'cloud_coverage', 'dew_temperature',\n",
    "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
    "       'wind_speed', 'meter_reading']\n",
    "\n",
    "#Include row_id. All columns except meter_reading\n",
    "test_ordered_column_names = ['row_id', 'site_id', 'building_id', 'timestamp', 'meter',\n",
    "       'primary_use', 'square_feet', 'year_built', 'floor_count',\n",
    "       'air_temperature', 'cloud_coverage', 'dew_temperature',\n",
    "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
    "       'wind_speed']\n",
    "\n",
    "# Order the column names in convenient order\n",
    "train_df = train_df[train_ordered_column_names]\n",
    "test_df = test_df[test_ordered_column_names]\n",
    "print(f'Shape of train : {train_df.shape}')\n",
    "print(f'Shape of train : {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 16s, sys: 13.8 s, total: 3min 30s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Sort train and test based on time\n",
    "train_df.sort_values(['timestamp', 'site_id', 'building_id'], inplace=True)\n",
    "test_df.sort_values(['timestamp', 'site_id', 'building_id'], inplace=True)\n",
    "\n",
    "# Do label encoding for the String type of feature\n",
    "feature_name = 'primary_use'\n",
    "lb = LabelEncoder()\n",
    "lb.fit(list(train_df[feature_name].values) + list(test_df[feature_name].values))\n",
    "train_df[feature_name] = lb.transform(list(train_df[feature_name].values))\n",
    "test_df[feature_name] = lb.transform(list(test_df[feature_name].values))\n",
    "\n",
    "# Add date related features.\n",
    "train_df = create_date_features(train_df, 'timestamp')\n",
    "test_df = create_date_features(test_df, 'timestamp')\n",
    "\n",
    "# Remove time stamp and meter reading\n",
    "predictors = ['site_id', 'building_id', 'meter', 'primary_use',\n",
    "       'square_feet', 'year_built', 'floor_count', 'air_temperature',\n",
    "       'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr',\n",
    "       'sea_level_pressure', 'wind_direction', 'wind_speed',\n",
    "       'year', 'month', 'quarter', 'weekofyear', 'day', 'dayofweek',\n",
    "       'dayofyear', 'is_month_start', 'is_month_end', 'is_quarter_start',\n",
    "       'is_quarter_end', 'is_year_start', 'is_year_end', 'hour', 'minute',\n",
    "       'second']\n",
    "target =  'meter_reading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X : (20216100, 30)\n",
      "Shape of train_Y : (20216100,)\n",
      "Shape of test_X : (41697600, 30)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_df[predictors]\n",
    "train_Y = np.log1p(train_df[target])\n",
    "test_X = test_df[predictors]\n",
    "\n",
    "print(f'Shape of train_X : {train_X.shape}')\n",
    "print(f'Shape of train_Y : {train_Y.shape}')\n",
    "print(f'Shape of test_X : {test_X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=100, n_jobs=16, num_leaves=31, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "reg = lgb.LGBMRegressor(n_jobs=16)\n",
    "reg.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3164"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_X, train_Y, train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "prediction = reg.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "prediction = np.expm1(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/envs/py37/lib/python3.7/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    }
   ],
   "source": [
    "submission_df =  feather.read_dataframe(f'{CREATED_DATA_DIR}/submission.feather')\n",
    "\n",
    "submission_df.row_id = test_df.row_id\n",
    "\n",
    "submission_df.meter_reading = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.5G\n",
      "-rw-r--r-- 1 jupyter jupyter  12K Oct 25 04:30 submission_2.ipynb\n",
      "-rw-r--r-- 1 jupyter jupyter 441M Oct 25 04:29 submission_2.zip\n",
      "-rw-r--r-- 1 jupyter jupyter 1.1G Oct 25 04:28 submission_2.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: submission_2.csv (deflated 59%)\n"
     ]
    }
   ],
   "source": [
    "! zip submission_2.zip submission_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf submission_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441M/441M [00:19<00:00, 24.2MB/s] \n",
      "Successfully submitted to ASHRAE - Great Energy Predictor III"
     ]
    }
   ],
   "source": [
    "! /home/jupyter/.local/bin/kaggle competitions submit -c ashrae-energy-prediction -f submission_2.zip -m \"On top of submission_1 just applied log1p on meter_reading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_37",
   "language": "python",
   "name": "py_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
